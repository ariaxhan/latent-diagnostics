{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT CONTEXT\n",
    "\n",
    "**These results are SUPERSEDED by length-controlled analysis.**\n",
    "\n",
    "This notebook was part of our early exploration of prompt injection detection. The apparent signal we found was largely driven by **text length confounding**:\n",
    "\n",
    "- `n_active` (feature count) correlates r=0.96+ with text length\n",
    "- After regressing out length, injection detection collapses to d~0.1\n",
    "- The \"geometry\" differences we observed were mostly longer-texts-activate-more-features\n",
    "\n",
    "**What we learned:**\n",
    "1. Raw feature counts are unreliable - they scale with input length\n",
    "2. True diagnostic signal requires length-controlled metrics (influence, concentration)\n",
    "3. Task-type detection works; injection-as-separate-category does not\n",
    "\n",
    "**Current approach:** See main `notebooks/` folder for length-controlled analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Truth About Injection Geometry\n",
    "\n",
    "## Critical Finding: The Length Confound\n",
    "\n",
    "**Previous claim:** Injection prompts have a distinct \"geometric signature\" - more features, lower concentration, weaker connections.\n",
    "\n",
    "**Reality:** This signature is largely a **length artifact**.\n",
    "\n",
    "| Finding | Original Analysis | Length-Controlled |\n",
    "|---------|-------------------|-------------------|\n",
    "| n_active | d=1.1, p<0.001*** | d=0.5, p=0.12 |\n",
    "| concentration | d=1.2, p<0.001*** | d=0.3, p=0.45 |\n",
    "| mean_influence | d=1.3, p<0.001*** | d=0.3, p=0.16 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "with open('../data/results/pint_attribution_metrics.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "samples = data['samples']\n",
    "injections = [s for s in samples if s['label']]\n",
    "benigns = [s for s in samples if not s['label']]\n",
    "\n",
    "print(f\"Dataset: {len(samples)} samples\")\n",
    "print(f\"Injection: {len(injections)} | Benign: {len(benigns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confound: Length → Everything\n",
    "\n",
    "**Key insight:** Prompt length correlates almost perfectly (r=0.96) with number of active features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length statistics\n",
    "inj_lens = [len(s['text']) for s in injections]\n",
    "ben_lens = [len(s['text']) for s in benigns]\n",
    "\n",
    "print(\"LENGTH STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Injection: {np.mean(inj_lens):.0f} ± {np.std(inj_lens):.0f} chars\")\n",
    "print(f\"Benign: {np.mean(ben_lens):.0f} ± {np.std(ben_lens):.0f} chars\")\n",
    "print(f\"Difference: {100*(np.mean(inj_lens)/np.mean(ben_lens) - 1):.0f}% longer\")\n",
    "print()\n",
    "\n",
    "# Correlation with metrics\n",
    "all_lens = [len(s['text']) for s in samples]\n",
    "print(\"CORRELATIONS WITH LENGTH\")\n",
    "print(\"-\" * 50)\n",
    "for metric in ['n_active', 'n_edges', 'top_100_concentration', 'mean_influence']:\n",
    "    vals = [s.get(metric, 0) for s in samples]\n",
    "    r, p = stats.pearsonr(all_lens, vals)\n",
    "    print(f\"{metric:<25} r={r:+.3f} (p={p:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: The confound\n",
    "ax1 = axes[0]\n",
    "ax1.scatter([len(s['text']) for s in benigns], \n",
    "            [s['n_active'] for s in benigns],\n",
    "            c='green', alpha=0.6, s=60, label='Benign')\n",
    "ax1.scatter([len(s['text']) for s in injections], \n",
    "            [s['n_active'] for s in injections],\n",
    "            c='red', alpha=0.8, s=80, marker='X', label='Injection')\n",
    "\n",
    "# Regression line\n",
    "all_lens = np.array([len(s['text']) for s in samples])\n",
    "all_active = np.array([s['n_active'] for s in samples])\n",
    "z = np.polyfit(all_lens, all_active, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(all_lens.min(), all_lens.max(), 100)\n",
    "ax1.plot(x_line, p(x_line), 'k--', linewidth=2, label=f'r=0.96')\n",
    "\n",
    "ax1.set_xlabel('Prompt Length (chars)', fontsize=12)\n",
    "ax1.set_ylabel('Number of Active Features', fontsize=12)\n",
    "ax1.set_title('THE CONFOUND\\nLength nearly perfectly predicts features', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Length distributions\n",
    "ax2 = axes[1]\n",
    "ax2.hist(ben_lens, bins=20, alpha=0.6, label=f'Benign (μ={np.mean(ben_lens):.0f})', color='green', density=True)\n",
    "ax2.hist(inj_lens, bins=15, alpha=0.6, label=f'Injection (μ={np.mean(inj_lens):.0f})', color='red', density=True)\n",
    "ax2.axvline(np.mean(ben_lens), color='darkgreen', linestyle='--', linewidth=2)\n",
    "ax2.axvline(np.mean(inj_lens), color='darkred', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Prompt Length (chars)', fontsize=12)\n",
    "ax2.set_ylabel('Density', fontsize=12)\n",
    "ax2.set_title('Injections are 74% LONGER\\nThis explains the \"geometric signature\"', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/length_confound.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length-Controlled Analysis\n",
    "\n",
    "**Method:** Match each injection to a benign prompt of similar length (±30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_match(inj_list, ben_list, tolerance=0.3):\n",
    "    \"\"\"Match injections to benign prompts of similar length.\"\"\"\n",
    "    matched_inj = []\n",
    "    matched_ben = []\n",
    "    remaining = ben_list.copy()\n",
    "    \n",
    "    for inj in inj_list:\n",
    "        inj_len = len(inj['text'])\n",
    "        candidates = [b for b in remaining \n",
    "                      if abs(len(b['text']) - inj_len) / inj_len < tolerance]\n",
    "        if candidates:\n",
    "            match = random.choice(candidates)\n",
    "            matched_inj.append(inj)\n",
    "            matched_ben.append(match)\n",
    "            remaining.remove(match)\n",
    "    \n",
    "    return matched_inj, matched_ben\n",
    "\n",
    "matched_inj, matched_ben = length_match(injections, benigns)\n",
    "\n",
    "print(f\"Matched pairs: {len(matched_inj)}\")\n",
    "print(f\"Injection length: {np.mean([len(s['text']) for s in matched_inj]):.0f} chars\")\n",
    "print(f\"Benign length: {np.mean([len(s['text']) for s in matched_ben]):.0f} chars\")\n",
    "print()\n",
    "\n",
    "# Compare metrics\n",
    "print(\"LENGTH-CONTROLLED COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'Injection':<12} {'Benign':<12} {'Cohen d':<10} {'p-value':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for metric in ['n_active', 'top_100_concentration', 'mean_influence']:\n",
    "    inj_vals = np.array([s.get(metric, 0) for s in matched_inj])\n",
    "    ben_vals = np.array([s.get(metric, 0) for s in matched_ben])\n",
    "    \n",
    "    inj_vals = inj_vals[inj_vals > 0]\n",
    "    ben_vals = ben_vals[ben_vals > 0]\n",
    "    \n",
    "    pooled = np.sqrt((inj_vals.std()**2 + ben_vals.std()**2) / 2)\n",
    "    d = (inj_vals.mean() - ben_vals.mean()) / pooled if pooled > 0 else 0\n",
    "    _, p = stats.mannwhitneyu(inj_vals, ben_vals)\n",
    "    \n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "    print(f\"{metric:<25} {inj_vals.mean():<12.4f} {ben_vals.mean():<12.4f} {d:<10.3f} {p:<10.4f} {sig}\")\n",
    "\n",
    "print()\n",
    "print(\"⚠️ NO SIGNIFICANT DIFFERENCES when controlling for length!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Original (misleading)\n",
    "ax1 = axes[0]\n",
    "ax1.scatter([s['n_active'] for s in benigns], \n",
    "            [s['top_100_concentration'] for s in benigns],\n",
    "            c='green', alpha=0.5, s=50, label='Benign (all)')\n",
    "ax1.scatter([s['n_active'] for s in injections], \n",
    "            [s['top_100_concentration'] for s in injections],\n",
    "            c='red', alpha=0.7, s=80, marker='X', label='Injection')\n",
    "ax1.set_xlabel('Number of Active Features', fontsize=12)\n",
    "ax1.set_ylabel('Top-100 Concentration', fontsize=12)\n",
    "ax1.set_title('ORIGINAL ANALYSIS\\n\"Clear separation\" (but length-confounded)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Length-matched (truth)\n",
    "ax2 = axes[1]\n",
    "ax2.scatter([s['n_active'] for s in matched_ben], \n",
    "            [s['top_100_concentration'] for s in matched_ben],\n",
    "            c='green', alpha=0.7, s=80, label=f'Benign (matched, n={len(matched_ben)})')\n",
    "ax2.scatter([s['n_active'] for s in matched_inj], \n",
    "            [s['top_100_concentration'] for s in matched_inj],\n",
    "            c='red', alpha=0.7, s=80, marker='X', label=f'Injection (n={len(matched_inj)})')\n",
    "\n",
    "# Centroids\n",
    "ax2.scatter(np.mean([s['n_active'] for s in matched_ben]), \n",
    "            np.mean([s['top_100_concentration'] for s in matched_ben]),\n",
    "            c='darkgreen', s=250, marker='s', edgecolors='white', linewidth=2, zorder=10)\n",
    "ax2.scatter(np.mean([s['n_active'] for s in matched_inj]), \n",
    "            np.mean([s['top_100_concentration'] for s in matched_inj]),\n",
    "            c='darkred', s=250, marker='s', edgecolors='white', linewidth=2, zorder=10)\n",
    "\n",
    "ax2.set_xlabel('Number of Active Features', fontsize=12)\n",
    "ax2.set_ylabel('Top-100 Concentration', fontsize=12)\n",
    "ax2.set_title('LENGTH-CONTROLLED\\nNo significant separation (p>0.1)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('The \"Geometric Signature\" Was a Length Artifact', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/truth_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications\n",
    "\n",
    "### What This Means\n",
    "\n",
    "1. **The original hypothesis is not supported** by this data\n",
    "2. **Length is a confound** that must be controlled in any injection detection work\n",
    "3. **The geometric signature** (if real) requires length-controlled sampling to demonstrate\n",
    "\n",
    "### What We Need\n",
    "\n",
    "A new experiment with:\n",
    "- **Length-matched sampling** from the start\n",
    "- **Larger sample sizes** (50+ per class) for statistical power\n",
    "- **Pre-registration** to avoid p-hacking\n",
    "\n",
    "### Alternative Hypotheses\n",
    "\n",
    "If length explains feature count, what *could* differentiate injections?\n",
    "\n",
    "1. **Specific feature activations** (e.g., \"instruction override\" features)\n",
    "2. **Graph topology** (not just size, but structure)\n",
    "3. **Residual after length normalization** (weak but maybe real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we find any signal after aggressive length control?\n",
    "print(\"RESIDUAL ANALYSIS: After Length Normalization\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Normalize each metric by length\n",
    "for metric in ['n_active', 'top_100_concentration', 'mean_influence']:\n",
    "    inj_vals = np.array([s.get(metric, 0) / len(s['text']) for s in injections])\n",
    "    ben_vals = np.array([s.get(metric, 0) / len(s['text']) for s in benigns])\n",
    "    \n",
    "    inj_vals = inj_vals[~np.isnan(inj_vals) & (inj_vals != 0)]\n",
    "    ben_vals = ben_vals[~np.isnan(ben_vals) & (ben_vals != 0)]\n",
    "    \n",
    "    pooled = np.sqrt((inj_vals.std()**2 + ben_vals.std()**2) / 2)\n",
    "    d = (inj_vals.mean() - ben_vals.mean()) / pooled if pooled > 0 else 0\n",
    "    _, p = stats.mannwhitneyu(inj_vals, ben_vals)\n",
    "    \n",
    "    sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "    print(f\"{metric}/length: d={d:.3f}, p={p:.4f} {sig}\")\n",
    "\n",
    "print()\n",
    "print(\"Note: n_active/length shows weak residual signal (d~0.8)\")\n",
    "print(\"This suggests injections *may* have slightly higher feature density,\")\n",
    "print(\"but effect is much weaker than originally claimed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps: Proper Experiment Design\n",
    "\n",
    "### Option A: Length-Matched Rerun\n",
    "\n",
    "```bash\n",
    "modal run scripts/modal_length_matched.py --n-per-class 50\n",
    "```\n",
    "\n",
    "This samples prompts ensuring:\n",
    "- 50 injections, 50 benign\n",
    "- Mean length within 10% between classes\n",
    "- Random selection within length constraints\n",
    "\n",
    "### Option B: Feature-Level Analysis\n",
    "\n",
    "Instead of graph-level metrics, look for:\n",
    "- Specific features that activate only for injections\n",
    "- \"Instruction override\" or \"ignore\" concepts\n",
    "- Semantic content, not just graph size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"ORIGINAL CLAIM:\")\n",
    "print(\"  Injections have distinct geometry (more features, lower concentration)\")\n",
    "print()\n",
    "print(\"REALITY:\")\n",
    "print(\"  Injections are 74% longer → more features → lower concentration\")\n",
    "print(\"  When controlling for length, no significant differences remain\")\n",
    "print()\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"  The 'geometric signature' was largely a length artifact\")\n",
    "print(\"  Need length-controlled experiment for valid claims\")\n",
    "print()\n",
    "print(\"WEAK RESIDUAL SIGNAL:\")\n",
    "print(\"  Features per character slightly higher in injections (d~0.8)\")\n",
    "print(\"  This may be worth investigating with proper controls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
