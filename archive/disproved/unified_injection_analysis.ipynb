{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT CONTEXT\n",
    "\n",
    "**These results are SUPERSEDED by length-controlled analysis.**\n",
    "\n",
    "This notebook was part of our early exploration of prompt injection detection. The apparent signal we found was largely driven by **text length confounding**:\n",
    "\n",
    "- `n_active` (feature count) correlates r=0.96+ with text length\n",
    "- After regressing out length, injection detection collapses to d~0.1\n",
    "- The \"geometry\" differences we observed were mostly longer-texts-activate-more-features\n",
    "\n",
    "**What we learned:**\n",
    "1. Raw feature counts are unreliable - they scale with input length\n",
    "2. True diagnostic signal requires length-controlled metrics (influence, concentration)\n",
    "3. Task-type detection works; injection-as-separate-category does not\n",
    "\n",
    "**Current approach:** See main `notebooks/` folder for length-controlled analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified Injection Geometry Analysis\n",
    "\n",
    "**Purpose:** Establish whether attribution graph geometry discriminates injection from benign prompts.\n",
    "\n",
    "**Critical Questions:**\n",
    "1. Does class balance affect the geometric signature?\n",
    "2. Why are injection datasets naturally imbalanced?\n",
    "3. Are geometric measurements independent across samples?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "\n",
    "### Three Conditions\n",
    "\n",
    "| Experiment | Injection | Benign | Ratio | Purpose |\n",
    "|------------|-----------|--------|-------|--------|\n",
    "| **Original** | 21 | 115 | 1:5.5 | Raw dataset distribution |\n",
    "| **Sanity Check** | 21 | 21 | 1:1 | Downsample benign, test if signal persists |\n",
    "| **Full Balanced** | 50 | 50 | 1:1 | Proper statistical power |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "**Imbalanced data creates illusions:**\n",
    "- Original 84.6% baseline (always predict benign)\n",
    "- Claimed 80.5% accuracy is WORSE than guessing majority class\n",
    "- Effect sizes can be inflated by outliers in small minority class\n",
    "\n",
    "**Balanced data reveals truth:**\n",
    "- 50% baseline (random guess)\n",
    "- Any accuracy above 50% is real signal\n",
    "- Effect sizes are fairly estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Unified Injection Geometry Analysis\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 1: Why Are Injection Datasets Imbalanced?\n",
    "\n",
    "### The Dataset Economics Problem\n",
    "\n",
    "**Benign prompts are cheap:**\n",
    "- Scraped from public conversations, forums, customer support logs\n",
    "- Generated synthetically (\"write me a poem about X\")\n",
    "- Abundant in any chatbot deployment\n",
    "\n",
    "**Injection prompts are expensive:**\n",
    "- Require security expertise to craft\n",
    "- Must be novel (known patterns get filtered)\n",
    "- Ethical constraints on collection\n",
    "- Adversarial evolution (attackers adapt)\n",
    "\n",
    "### Real-World Distribution\n",
    "\n",
    "In production, injection attempts are **rare** (< 1% of traffic). Datasets mirror this:\n",
    "\n",
    "| Dataset | Injection % | Source |\n",
    "|---------|-------------|--------|\n",
    "| deepset/prompt-injections | 37% | Research collection |\n",
    "| PINT Benchmark | 6% | Curated benchmark |\n",
    "| Production logs | ~0.1-1% | Real deployment |\n",
    "\n",
    "**Implication:** Benchmarks oversample injections to enable evaluation, but this creates statistical artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original experiment data\n",
    "data_path = Path('../data/results/pint_attribution_metrics.json')\n",
    "\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_samples = data['samples']\n",
    "injections = [s for s in all_samples if s['label']]\n",
    "benigns = [s for s in all_samples if not s['label']]\n",
    "\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total samples computed: {len(all_samples)}\")\n",
    "print(f\"Injection: {len(injections)} ({100*len(injections)/len(all_samples):.1f}%)\")\n",
    "print(f\"Benign: {len(benigns)} ({100*len(benigns)/len(all_samples):.1f}%)\")\n",
    "print(f\"Imbalance ratio: {len(benigns)/len(injections):.1f}:1\")\n",
    "print()\n",
    "print(f\"Source: {data['metadata']['dataset']}\")\n",
    "print(f\"Model: {data['metadata']['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Are Measurements Independent?\n",
    "\n",
    "### The Independence Assumption\n",
    "\n",
    "Statistical tests (t-test, Mann-Whitney) assume samples are **independent and identically distributed (i.i.d.)**.\n",
    "\n",
    "**For our experiment:**\n",
    "\n",
    "| Factor | Independent? | Reasoning |\n",
    "|--------|--------------|----------|\n",
    "| Model weights | ✅ Yes | Same frozen model, no learning |\n",
    "| Transcoder | ✅ Yes | Same SAE, deterministic |\n",
    "| Prompt text | ⚠️ Partially | Some prompts may share templates |\n",
    "| Graph computation | ✅ Yes | Each prompt processed separately |\n",
    "| GPU state | ✅ Yes | Cache cleared between samples |\n",
    "\n",
    "### Potential Violations\n",
    "\n",
    "1. **Template effects:** If injection prompts share \"Ignore previous instructions\" prefix, they're not truly independent\n",
    "2. **Length correlation:** Longer prompts → more features (confound)\n",
    "3. **Semantic clustering:** Similar topics activate similar features\n",
    "\n",
    "**We'll test for these below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for length confound\n",
    "inj_lengths = [len(s['text']) for s in injections]\n",
    "ben_lengths = [len(s['text']) for s in benigns]\n",
    "\n",
    "print(\"Independence Check: Prompt Length\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Injection mean length: {np.mean(inj_lengths):.0f} chars\")\n",
    "print(f\"Benign mean length: {np.mean(ben_lengths):.0f} chars\")\n",
    "\n",
    "# Correlation between length and key metrics\n",
    "all_lengths = [len(s['text']) for s in all_samples]\n",
    "all_n_active = [s.get('n_active', 0) for s in all_samples]\n",
    "\n",
    "corr, p = stats.pearsonr(all_lengths, all_n_active)\n",
    "print(f\"\\nCorrelation (length vs n_active): r={corr:.3f}, p={p:.4f}\")\n",
    "\n",
    "if abs(corr) > 0.5:\n",
    "    print(\"⚠️ WARNING: Strong length confound detected!\")\n",
    "    print(\"   Longer prompts have more features - need to control for length\")\n",
    "else:\n",
    "    print(\"✓ Length confound is weak - measurements appear independent of length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for template patterns in injections\n",
    "print(\"Independence Check: Injection Templates\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Common injection prefixes\n",
    "prefixes = {}\n",
    "for s in injections:\n",
    "    # Get first 20 chars as potential template\n",
    "    prefix = s['text'][:20].lower()\n",
    "    prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
    "\n",
    "# Count unique vs repeated\n",
    "unique = sum(1 for v in prefixes.values() if v == 1)\n",
    "repeated = sum(1 for v in prefixes.values() if v > 1)\n",
    "\n",
    "print(f\"Unique injection prefixes: {unique}\")\n",
    "print(f\"Repeated prefixes: {repeated}\")\n",
    "print(f\"Template diversity: {100*unique/len(injections):.0f}%\")\n",
    "\n",
    "if repeated > len(injections) * 0.3:\n",
    "    print(\"\\n⚠️ WARNING: Many injections share templates\")\n",
    "    print(\"   This violates independence assumption\")\n",
    "else:\n",
    "    print(\"\\n✓ Injections appear diverse - independence assumption reasonable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Does Balance Matter?\n",
    "\n",
    "We'll compare three conditions to see if the geometric signature holds across different sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_condition(inj_samples, ben_samples, condition_name):\n",
    "    \"\"\"\n",
    "    Analyze geometric metrics for a given condition.\n",
    "    Returns dict with effect sizes and p-values.\n",
    "    \"\"\"\n",
    "    metrics = ['n_active', 'n_edges', 'top_100_concentration', 'mean_influence']\n",
    "    results = {'condition': condition_name, 'n_inj': len(inj_samples), 'n_ben': len(ben_samples)}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        inj_vals = np.array([s.get(metric, 0) for s in inj_samples])\n",
    "        ben_vals = np.array([s.get(metric, 0) for s in ben_samples])\n",
    "        \n",
    "        # Filter invalid\n",
    "        inj_vals = inj_vals[~np.isnan(inj_vals) & (inj_vals != 0)]\n",
    "        ben_vals = ben_vals[~np.isnan(ben_vals) & (ben_vals != 0)]\n",
    "        \n",
    "        if len(inj_vals) < 3 or len(ben_vals) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Effect size\n",
    "        pooled_std = np.sqrt((inj_vals.std()**2 + ben_vals.std()**2) / 2)\n",
    "        cohen_d = (inj_vals.mean() - ben_vals.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "        \n",
    "        # Mann-Whitney U (robust to non-normality)\n",
    "        _, p_value = stats.mannwhitneyu(inj_vals, ben_vals, alternative='two-sided')\n",
    "        \n",
    "        results[f'{metric}_d'] = cohen_d\n",
    "        results[f'{metric}_p'] = p_value\n",
    "        results[f'{metric}_inj_mean'] = inj_vals.mean()\n",
    "        results[f'{metric}_ben_mean'] = ben_vals.mean()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_comparison(results_list):\n",
    "    \"\"\"Pretty-print comparison across conditions.\"\"\"\n",
    "    metrics = ['n_active', 'n_edges', 'top_100_concentration', 'mean_influence']\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"CROSS-CONDITION COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for metric in metrics:\n",
    "        print(f\"\\n{metric.upper()}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Condition':<25} {'n_inj':<8} {'n_ben':<8} {'Cohen d':<12} {'p-value':<12} {'Sig?'}\")\n",
    "        \n",
    "        for r in results_list:\n",
    "            d = r.get(f'{metric}_d', float('nan'))\n",
    "            p = r.get(f'{metric}_p', float('nan'))\n",
    "            sig = '***' if p < 0.001 else '**' if p < 0.01 else '*' if p < 0.05 else ''\n",
    "            print(f\"{r['condition']:<25} {r['n_inj']:<8} {r['n_ben']:<8} {d:<12.3f} {p:<12.4f} {sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition 1: Original (imbalanced)\n",
    "original = analyze_condition(injections, benigns, \"Original (1:5.5)\")\n",
    "\n",
    "# Condition 2: Sanity check (downsample benign)\n",
    "benigns_downsampled = random.sample(benigns, len(injections))\n",
    "sanity = analyze_condition(injections, benigns_downsampled, \"Sanity Check (1:1)\")\n",
    "\n",
    "# Condition 3: Different random downsample (robustness check)\n",
    "random.seed(123)  # Different seed\n",
    "benigns_alt = random.sample(benigns, len(injections))\n",
    "robust = analyze_condition(injections, benigns_alt, \"Robustness Check (1:1)\")\n",
    "\n",
    "# Compare all\n",
    "print_comparison([original, sanity, robust])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY: DOES BALANCE MATTER?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if significance holds across conditions\n",
    "metrics = ['n_active', 'n_edges', 'top_100_concentration', 'mean_influence']\n",
    "\n",
    "print(\"\\nSignificance (p < 0.05) across conditions:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25} {'Original':<12} {'Sanity':<12} {'Robust':<12}\")\n",
    "\n",
    "all_hold = True\n",
    "for metric in metrics:\n",
    "    orig_sig = original.get(f'{metric}_p', 1) < 0.05\n",
    "    san_sig = sanity.get(f'{metric}_p', 1) < 0.05\n",
    "    rob_sig = robust.get(f'{metric}_p', 1) < 0.05\n",
    "    \n",
    "    print(f\"{metric:<25} {'✓' if orig_sig else '✗':<12} {'✓' if san_sig else '✗':<12} {'✓' if rob_sig else '✗':<12}\")\n",
    "    \n",
    "    if not (san_sig and rob_sig):\n",
    "        all_hold = False\n",
    "\n",
    "print()\n",
    "if all_hold:\n",
    "    print(\"✅ CONCLUSION: Geometric signature HOLDS with balanced sampling\")\n",
    "    print(\"   The effect is real, not an artifact of class imbalance\")\n",
    "else:\n",
    "    print(\"⚠️ CONCLUSION: Some metrics lose significance with balanced sampling\")\n",
    "    print(\"   Effect may be partially inflated by imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization: Effect Size Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metrics = ['n_active', 'top_100_concentration', 'mean_influence']\n",
    "conditions = ['Original (1:5.5)', 'Sanity Check (1:1)', 'Robustness Check (1:1)']\n",
    "results_list = [original, sanity, robust]\n",
    "\n",
    "# Plot 1: Cohen's d across conditions\n",
    "ax1 = axes[0]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, (r, cond) in enumerate(zip(results_list, conditions)):\n",
    "    d_vals = [abs(r.get(f'{m}_d', 0)) for m in metrics]\n",
    "    ax1.bar(x + i*width, d_vals, width, label=cond, alpha=0.8)\n",
    "\n",
    "ax1.axhline(0.8, color='red', linestyle='--', label='Large effect threshold')\n",
    "ax1.set_ylabel('|Cohen\\'s d|', fontsize=12)\n",
    "ax1.set_title('Effect Size Stability Across Conditions', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels([m.replace('_', '\\n') for m in metrics])\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Scatter comparison\n",
    "ax2 = axes[1]\n",
    "\n",
    "random.seed(SEED)\n",
    "benigns_balanced = random.sample(benigns, len(injections))\n",
    "\n",
    "# Original (all benign)\n",
    "ax2.scatter([s['n_active'] for s in benigns], \n",
    "            [s['top_100_concentration'] for s in benigns],\n",
    "            c='lightgreen', alpha=0.3, s=30, label='Benign (all 115)')\n",
    "\n",
    "# Balanced benign (highlighted)\n",
    "ax2.scatter([s['n_active'] for s in benigns_balanced], \n",
    "            [s['top_100_concentration'] for s in benigns_balanced],\n",
    "            c='green', alpha=0.7, s=80, label='Benign (sampled 21)')\n",
    "\n",
    "# Injection\n",
    "ax2.scatter([s['n_active'] for s in injections], \n",
    "            [s['top_100_concentration'] for s in injections],\n",
    "            c='red', alpha=0.7, s=80, marker='X', label='Injection (21)')\n",
    "\n",
    "ax2.set_xlabel('Number of Active Features', fontsize=12)\n",
    "ax2.set_ylabel('Top-100 Concentration', fontsize=12)\n",
    "ax2.set_title('Sampling Strategy Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/effect_size_stability.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bootstrap Analysis: Confidence Intervals\n",
    "\n",
    "With small samples, we need bootstrap to estimate uncertainty in effect sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_cohen_d(inj_samples, ben_samples, metric, n_bootstrap=1000):\n",
    "    \"\"\"Bootstrap confidence interval for Cohen's d.\"\"\"\n",
    "    inj_vals = np.array([s.get(metric, 0) for s in inj_samples])\n",
    "    ben_vals = np.array([s.get(metric, 0) for s in ben_samples])\n",
    "    \n",
    "    # Filter\n",
    "    inj_vals = inj_vals[~np.isnan(inj_vals) & (inj_vals != 0)]\n",
    "    ben_vals = ben_vals[~np.isnan(ben_vals) & (ben_vals != 0)]\n",
    "    \n",
    "    d_samples = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        inj_boot = np.random.choice(inj_vals, size=len(inj_vals), replace=True)\n",
    "        ben_boot = np.random.choice(ben_vals, size=len(ben_vals), replace=True)\n",
    "        \n",
    "        pooled_std = np.sqrt((inj_boot.std()**2 + ben_boot.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            d = (inj_boot.mean() - ben_boot.mean()) / pooled_std\n",
    "            d_samples.append(d)\n",
    "    \n",
    "    return np.percentile(d_samples, [2.5, 50, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% CI for Cohen's d (balanced sampling)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'2.5%':<10} {'Median':<10} {'97.5%':<10} {'Significant?'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for metric in ['n_active', 'top_100_concentration', 'mean_influence']:\n",
    "    ci = bootstrap_cohen_d(injections, benigns_balanced, metric)\n",
    "    # Significant if CI doesn't include 0\n",
    "    sig = \"✓\" if (ci[0] > 0 or ci[2] < 0) else \"✗\"\n",
    "    print(f\"{metric:<25} {ci[0]:<10.3f} {ci[1]:<10.3f} {ci[2]:<10.3f} {sig}\")\n",
    "\n",
    "print(\"\\n(Significant = 95% CI excludes zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Power Analysis\n",
    "\n",
    "How many samples do we need for reliable results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def power_analysis(effect_size, n_per_group, alpha=0.05):\n",
    "    \"\"\"Calculate statistical power for two-sample t-test.\"\"\"\n",
    "    # Standard error of difference\n",
    "    se = np.sqrt(2 / n_per_group)\n",
    "    # Critical z value\n",
    "    z_crit = norm.ppf(1 - alpha/2)\n",
    "    # Non-centrality parameter\n",
    "    ncp = effect_size / se\n",
    "    # Power\n",
    "    power = 1 - norm.cdf(z_crit - ncp) + norm.cdf(-z_crit - ncp)\n",
    "    return power\n",
    "\n",
    "print(\"Statistical Power Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nAssumes Cohen's d ≈ 1.0 (observed for concentration metric)\")\n",
    "print()\n",
    "\n",
    "effect_size = 1.0\n",
    "sample_sizes = [10, 21, 30, 50, 100]\n",
    "\n",
    "print(f\"{'n per class':<15} {'Power':<15} {'Interpretation'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in sample_sizes:\n",
    "    pwr = power_analysis(effect_size, n)\n",
    "    if pwr < 0.5:\n",
    "        interp = \"Very weak - likely to miss real effects\"\n",
    "    elif pwr < 0.8:\n",
    "        interp = \"Underpowered - may miss effects\"\n",
    "    elif pwr < 0.95:\n",
    "        interp = \"Adequate power\"\n",
    "    else:\n",
    "        interp = \"High power - reliable detection\"\n",
    "    \n",
    "    marker = \"← Current\" if n == 21 else \"\"\n",
    "    print(f\"{n:<15} {pwr:<15.1%} {interp} {marker}\")\n",
    "\n",
    "print(\"\\n✓ Recommendation: 50 samples per class for 95% power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "### Does Balance Matter?\n",
    "\n",
    "**Yes, but the signal persists.** Effect sizes decrease slightly with balanced sampling, but remain statistically significant. This suggests the geometric signature is real, not an artifact of imbalance.\n",
    "\n",
    "### Why Are Datasets Imbalanced?\n",
    "\n",
    "**Economics + reality.** Benign prompts are abundant; injections require expertise to craft. Real-world traffic is < 1% injection, so benchmarks oversample to enable evaluation.\n",
    "\n",
    "### Are Measurements Independent?\n",
    "\n",
    "**Mostly yes, with caveats:**\n",
    "- Length is a weak confound (need to verify)\n",
    "- Template sharing in injections may violate independence\n",
    "- Each graph computation is truly independent\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Run balanced Modal experiment** (50 per class) for proper power\n",
    "2. **Control for prompt length** in analysis\n",
    "3. **Cross-validate** with different random seeds\n",
    "\n",
    "```bash\n",
    "# Run the balanced experiment\n",
    "modal run scripts/modal_balanced_benchmark.py --n-per-class 50\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Experiment Parameters\n",
    "\n",
    "### Original Experiment\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | deepset/prompt-injections |\n",
    "| Model | google/gemma-2-2b |\n",
    "| Transcoders | GemmaScope (gemma) |\n",
    "| Total samples | 136 (140 attempted, 4 failed) |\n",
    "| Class distribution | 21 injection, 115 benign |\n",
    "| Max tokens | 200 |\n",
    "| Attribution method | circuit-tracer |\n",
    "\n",
    "### Sanity Check\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Same data as original | Yes |\n",
    "| Benign downsampled to | 21 |\n",
    "| Random seed | 42 |\n",
    "| New computation | No (reused metrics) |\n",
    "\n",
    "### Planned Balanced Experiment\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | deepset/prompt-injections |\n",
    "| Samples per class | 50 |\n",
    "| Total samples | 100 |\n",
    "| Random seed | 42 |\n",
    "| GPU | A100 (Modal) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
