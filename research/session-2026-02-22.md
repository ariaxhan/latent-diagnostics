# Research Session: 2026-02-22

## Objective

Expand from injection detection to broader mechanistic interpretability research using SAE attribution analysis.

## Key Accomplishments

### 1. Diagnostic Suite on Existing Data

Ran comprehensive diagnostics (A-G) on 136 PINT samples:

**Key Findings:**
- **Length confound confirmed**: r=0.96 correlation between `n_active` and text length
- **Best metric**: `mean_activation` (AUC=0.830) has weak length correlation (r=-0.224)
- **Partial correlations**: Signal preserved after length control for most metrics
- **Underpowered**: Only 21 injection samples → wide confidence intervals

**Output:** `experiments/diagnostics/runs/20260222_110302/`

### 2. Dataset Infrastructure

Created `src/neural_polygraph/datasets.py` with loaders for:

| Dataset | Purpose | Samples |
|---------|---------|---------|
| TruthfulQA | Truthful vs false answers | 817 |
| HaluEval | Hallucinated vs faithful | 35K |
| CoLA | Grammatical acceptability | 10K |
| PAWS | Paraphrase adversaries | 108K |
| SNLI | Natural language inference | 570K |
| WinoGrande | Commonsense coreference | 44K |
| HellaSwag | Situational commonsense | 70K |
| CodeSearchNet | Code-docstring pairs | 2M |
| PubMedQA | Biomedical QA | 273K |
| Poem Sentiment | Poetry with sentiment | 892 |

### 3. Prepared Datasets for Attribution

**Domain Analysis** (`data/domain_analysis/domain_samples.json`):
- 400 samples across domains: grammar, commonsense, situational, inference, paraphrase, scientific, poetry, truthfulness
- Ready for Modal attribution run

**Truthfulness Analysis** (`data/truthfulness/truthfulness_samples.json`):
- 200 samples from TruthfulQA
- Balanced: 99 false, 101 truthful
- Ready for Modal attribution run

### 4. Experiment Scripts

- `experiments/domain_analysis.py`: Domain signature analysis
- `experiments/truthfulness_analysis.py`: Truthful vs false patterns
- `scripts/modal_general_attribution.py`: General-purpose Modal runner

---

## Next Steps: Modal Runs Required

### Run 1: Domain Analysis
```bash
modal run scripts/modal_general_attribution.py \
    --input-file data/domain_analysis/domain_samples.json \
    --output-file data/results/domain_attribution_metrics.json
```

Estimated time: ~1 hour (400 samples × ~10s each on A100)

### Run 2: Truthfulness Analysis
```bash
modal run scripts/modal_general_attribution.py \
    --input-file data/truthfulness/truthfulness_samples.json \
    --output-file data/results/truthfulness_metrics.json
```

Estimated time: ~30 min (200 samples)

---

## Research Questions to Explore

### Domain Signatures
1. Do different text domains activate fundamentally different feature subspaces?
2. Can we identify "code features" vs "prose features" vs "scientific features"?
3. Is there a domain-invariant "core" that activates across all text types?

### Truthfulness Patterns
1. Do truthful vs false statements have different activation density?
2. Is there a "truth feature" or "doubt feature" that fires differently?
3. How does this relate to hallucination detection?
4. Can activation patterns predict factual accuracy?

### Mechanistic Insights
1. Which SAE features correlate with specific text properties?
2. What is the "shape" of activation space for different content types?
3. Can we interpret individual high-activation features?

---

## Files Changed This Session

```
experiments/diagnostics.py              # Diagnostic suite
experiments/diagnostics/runs/           # Run output with figures
experiments/domain_analysis.py          # Domain signature experiment
experiments/truthfulness_analysis.py    # Truthfulness experiment
scripts/modal_general_attribution.py    # General Modal runner
src/neural_polygraph/datasets.py        # Dataset loaders
data/domain_analysis/domain_samples.json
data/truthfulness/truthfulness_samples.json
```

---

## Adversary Review (Previous Session)

From AgentDB checkpoint:
- Class imbalance: 15.4% injection → 84.6% baseline by always predicting benign
- Only 21 injection samples → weak statistical power
- 80.5% claimed accuracy is NOT competitive with 95% baselines

**Resolution:** Shifted focus from classification to mechanistic exploration. No longer trying to "prove" detection works - instead exploring what SAE attribution reveals about text.

---

## Handoff Notes

1. **Modal runs are the bottleneck**: Need GPU time for attribution
2. **Length confound**: Any claims must control for text length
3. **Small sample sizes**: Expand datasets before making claims
4. **Mechanistic focus**: We're exploring, not classifying
