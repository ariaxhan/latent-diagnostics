{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: What Doesn't Work (And Why That Matters)\n",
    "\n",
    "**Series**: Latent Diagnostics Analysis (5 of 5)\n",
    "\n",
    "Negative results are as important as positive results. This notebook documents what we tried and failed, why it failed, and what that teaches us about the limits of activation topology.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Truthfulness Detection: d = 0.05](#1-truthfulness-detection-d--005)\n",
    "2. [Why Truthfulness Fails](#2-why-truthfulness-fails)\n",
    "3. [Injection Detection: Collapsed After Length Control](#3-injection-detection-collapsed-after-length-control)\n",
    "4. [Hallucination Detection: Never Worked](#4-hallucination-detection-never-worked)\n",
    "5. [The Honest Summary](#5-the-honest-summary)\n",
    "6. [What This Teaches Us](#6-what-this-teaches-us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import linregress\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/results')\n",
    "FIGURES_DIR = Path('../figures/paper')\n",
    "\n",
    "# Style\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def residualize(metric, length):\n",
    "    \"\"\"Remove length effect from metric.\"\"\"\n",
    "    slope, intercept, _, _, _ = linregress(length, metric)\n",
    "    return metric - (slope * length + intercept)\n",
    "\n",
    "def cohens_d(a, b):\n",
    "    \"\"\"Cohen's d effect size.\"\"\"\n",
    "    na, nb = len(a), len(b)\n",
    "    pooled_std = np.sqrt(((na-1)*np.std(a, ddof=1)**2 + (nb-1)*np.std(b, ddof=1)**2) / (na+nb-2))\n",
    "    if pooled_std == 0:\n",
    "        return 0\n",
    "    return (np.mean(a) - np.mean(b)) / pooled_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Truthfulness Detection: d = 0.05\n",
    "\n",
    "**Hypothesis**: True and false statements might produce different activation patterns.\n",
    "\n",
    "**Result**: They look identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truthfulness data\n",
    "with open(DATA_DIR / 'truthfulness_metrics_clean.json') as f:\n",
    "    truth_data = json.load(f)\n",
    "\n",
    "df_truth = pd.DataFrame(truth_data['samples'])\n",
    "df_truth['text_length'] = df_truth['text'].str.len()\n",
    "\n",
    "print(f\"Truthfulness dataset: {len(df_truth)} samples\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df_truth['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the truthfulness overlap figure\n",
    "fig_path = FIGURES_DIR / 'truthfulness_overlap.png'\n",
    "if fig_path.exists():\n",
    "    display(Image(filename=str(fig_path), width=700))\n",
    "else:\n",
    "    print(f\"Figure not found: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute effect sizes for true vs false\n",
    "# Note: labels may be 'truthful'/'false' or 'true'/'false'\n",
    "true_labels = ['true', 'truthful']\n",
    "false_labels = ['false']\n",
    "\n",
    "true_mask = df_truth['label'].str.lower().isin(true_labels)\n",
    "false_mask = df_truth['label'].str.lower().isin(false_labels)\n",
    "\n",
    "df_true = df_truth[true_mask]\n",
    "df_false = df_truth[false_mask]\n",
    "\n",
    "print(f\"True statements: {len(df_true)}\")\n",
    "print(f\"False statements: {len(df_false)}\")\n",
    "\n",
    "# Add residualized metrics\n",
    "metrics = ['n_active', 'mean_influence', 'top_100_concentration', 'mean_activation']\n",
    "for m in metrics:\n",
    "    if m in df_truth.columns:\n",
    "        df_truth[f'{m}_resid'] = residualize(df_truth[m].values, df_truth['text_length'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update masks after adding residuals\n",
    "df_true = df_truth[true_mask]\n",
    "df_false = df_truth[false_mask]\n",
    "\n",
    "print(\"\\nEffect Sizes: True vs False Statements\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'Raw d':>12} {'Controlled d':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for m in metrics:\n",
    "    if m in df_true.columns:\n",
    "        d_raw = cohens_d(df_true[m].values, df_false[m].values)\n",
    "        d_resid = cohens_d(df_true[f'{m}_resid'].values, df_false[f'{m}_resid'].values)\n",
    "        print(f\"{m:<25} {d_raw:>+12.2f} {d_resid:>+15.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nAll effect sizes are tiny (d < 0.2). No detectable difference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the overlap\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, metric in zip(axes, ['mean_influence', 'top_100_concentration']):\n",
    "    if metric in df_true.columns:\n",
    "        true_vals = df_true[metric].values\n",
    "        false_vals = df_false[metric].values\n",
    "        \n",
    "        # KDE\n",
    "        x_min = min(true_vals.min(), false_vals.min())\n",
    "        x_max = max(true_vals.max(), false_vals.max())\n",
    "        x_range = np.linspace(x_min, x_max, 200)\n",
    "        \n",
    "        kde_true = gaussian_kde(true_vals)\n",
    "        kde_false = gaussian_kde(false_vals)\n",
    "        \n",
    "        ax.plot(x_range, kde_true(x_range), label='True', color='#27ae60', linewidth=2)\n",
    "        ax.fill_between(x_range, kde_true(x_range), alpha=0.3, color='#27ae60')\n",
    "        ax.plot(x_range, kde_false(x_range), label='False', color='#c0392b', linewidth=2)\n",
    "        ax.fill_between(x_range, kde_false(x_range), alpha=0.3, color='#c0392b')\n",
    "        \n",
    "        d = cohens_d(true_vals, false_vals)\n",
    "        ax.set_title(f'{metric}\\nd = {d:.2f} (no difference)')\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Truthfulness: Distributions Completely Overlap', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Why Truthfulness Fails\n",
    "\n",
    "### The Fundamental Issue\n",
    "\n",
    "The model processes \"The sky is blue\" and \"The sky is green\" **identically**.\n",
    "\n",
    "Both sentences:\n",
    "- Have the same grammatical structure\n",
    "- Require the same type of semantic processing\n",
    "- Activate similar features for similar reasons\n",
    "\n",
    "The only difference is **which specific knowledge** gets retrieved, not **how** the computation proceeds.\n",
    "\n",
    "### What Activation Topology Measures\n",
    "\n",
    "Activation topology measures **HOW** the model computes:\n",
    "- Feature count\n",
    "- Connection strength\n",
    "- Influence concentration\n",
    "\n",
    "It does NOT measure **WHETHER** the computation is correct.\n",
    "\n",
    "### Analogy\n",
    "\n",
    "This is like measuring brain activity during mental arithmetic:\n",
    "- \"2 + 2 = 4\" and \"2 + 2 = 5\" produce the same *type* of brain activity\n",
    "- The same regions activate, the same processes run\n",
    "- The only difference is which memory gets retrieved at the end\n",
    "\n",
    "**Truthfulness is a property of the output, not the computation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Injection Detection: Collapsed After Length Control\n",
    "\n",
    "**Original claim**: Adversarial injections have distinct activation patterns (d > 1.0).\n",
    "\n",
    "**After length control**: d ~ 0.1 (collapsed).\n",
    "\n",
    "### What Happened\n",
    "\n",
    "Injections are typically **longer** than normal prompts (they contain extra malicious text). The \"signal\" we detected was just **text length**, not a special \"injection signature.\"\n",
    "\n",
    "### The Lesson\n",
    "\n",
    "\"Injection\" isn't a coherent computational category. An injection is defined by **intent** (malicious use), not by any intrinsic property of the computation.\n",
    "\n",
    "**Full details**: See `archive/disproved/` in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of injection results (from archived analysis)\n",
    "injection_results = {\n",
    "    'Metric': ['n_active', 'mean_influence', 'concentration'],\n",
    "    'Raw d': [1.8, 1.2, 0.9],\n",
    "    'Length-Controlled d': [0.12, 0.08, 0.11],\n",
    "    'Status': ['COLLAPSED', 'COLLAPSED', 'COLLAPSED']\n",
    "}\n",
    "\n",
    "df_injection = pd.DataFrame(injection_results)\n",
    "print(\"Injection Detection Results (from archived analysis)\")\n",
    "print(\"=\" * 60)\n",
    "df_injection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Hallucination Detection: Never Worked\n",
    "\n",
    "**Original motivation**: This was the original reason for starting this project.\n",
    "\n",
    "**Result**: No signal found in early experiments.\n",
    "\n",
    "### Why It Failed\n",
    "\n",
    "Hallucinations are **outputs**, not **computations**.\n",
    "\n",
    "When a model hallucinates:\n",
    "- It runs the same type of computation as normal\n",
    "- It retrieves and combines features the same way\n",
    "- It just happens to output something that doesn't match reality\n",
    "\n",
    "The model doesn't \"know\" it's hallucinating. From its internal perspective, it's just doing normal text generation.\n",
    "\n",
    "### The Honest Acknowledgment\n",
    "\n",
    "This project started with hallucination detection as the goal. We pivoted to task-type classification after hallucination detection failed to show any signal.\n",
    "\n",
    "**The positive results we report (task-type classification) came from exploring what DOES work, not from the original hypothesis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Honest Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = {\n",
    "    'Task': ['Task type classification', 'Complexity estimation', 'Truthfulness detection', 'Injection detection', 'Hallucination detection'],\n",
    "    'Raw d': [3.22, 2.36, 0.08, 1.2, '~0'],\n",
    "    'Length-Controlled d': [1.08, 0.87, 0.05, '~0.1', '~0'],\n",
    "    'Works?': ['YES', 'YES', 'NO', 'NO', 'NO'],\n",
    "    'Why': [\n",
    "        'Different task types require different computation structures',\n",
    "        'Focused vs diffuse processing is measurable',\n",
    "        'Model processes true/false statements identically',\n",
    "        'Signal was just text length, not injection signature',\n",
    "        'Hallucination is an output property, not computation property'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"Complete Results Summary\")\n",
    "print(\"=\" * 100)\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the summary\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "tasks = ['Task Type', 'Complexity', 'Truthfulness', 'Injection', 'Hallucination']\n",
    "d_values = [1.08, 0.87, 0.05, 0.1, 0.0]\n",
    "colors = ['#27ae60' if d > 0.5 else '#c0392b' for d in d_values]\n",
    "\n",
    "bars = ax.barh(tasks, d_values, color=colors, edgecolor='black')\n",
    "ax.axvline(0.5, color='gray', linestyle='--', alpha=0.7, label='Medium effect threshold')\n",
    "ax.axvline(0.8, color='gray', linestyle=':', alpha=0.7, label='Large effect threshold')\n",
    "\n",
    "ax.set_xlabel(\"Cohen's d (length-controlled)\")\n",
    "ax.set_title('What Works and What Does Not', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Add value labels\n",
    "for bar, d in zip(bars, d_values):\n",
    "    status = 'WORKS' if d > 0.5 else 'FAILS'\n",
    "    ax.text(bar.get_width() + 0.05, bar.get_y() + bar.get_height()/2,\n",
    "            f'd={d:.2f} ({status})', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. What This Teaches Us\n",
    "\n",
    "### The Core Insight\n",
    "\n",
    "**Activation topology measures computation type, not output quality.**\n",
    "\n",
    "It can detect:\n",
    "- What *kind* of thinking is happening (grammar vs. reasoning)\n",
    "- How *focused* or *diffuse* the computation is\n",
    "\n",
    "It cannot detect:\n",
    "- Whether the output is correct\n",
    "- Whether the input is malicious\n",
    "- Whether the model is confident in the right way\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Prevents false claims**: We don't claim to detect things we can't detect.\n",
    "\n",
    "2. **Guides future research**: If you want to detect truthfulness or hallucinations, look elsewhere (output probing, uncertainty quantification, etc.).\n",
    "\n",
    "3. **Defines the tool**: Activation topology is for understanding **how** models compute, not for verifying **what** they output.\n",
    "\n",
    "### The Takeaway\n",
    "\n",
    "Internal structure is not the same as external correctness. A confident liar and a confident truth-teller have the same brain activity patterns. So do truthful and hallucinating language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This notebook documented three negative results:\n",
    "\n",
    "1. **Truthfulness**: d = 0.05 (no signal)\n",
    "2. **Injection detection**: Collapsed after length control\n",
    "3. **Hallucination detection**: Never showed signal\n",
    "\n",
    "These failures teach us what activation topology can and cannot do. It measures **computation type**, not **output correctness**.\n",
    "\n",
    "**The honest summary**: We can tell what kind of thinking a model is doing. We cannot tell if that thinking is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
