{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: What We Found - Task Types Have Distinct Signatures\n",
    "\n",
    "**Series**: Latent Diagnostics Analysis (4 of 5)\n",
    "\n",
    "This notebook presents the core positive results: different task types produce measurably different activation patterns, and these differences persist after controlling for text length.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Load and Prepare Data](#1-load-and-prepare-data)\n",
    "2. [The Five Task Types](#2-the-five-task-types)\n",
    "3. [Raw vs Length-Controlled Comparison](#3-raw-vs-length-controlled-comparison)\n",
    "4. [The Gradient from Focused to Diffuse](#4-the-gradient-from-focused-to-diffuse)\n",
    "5. [Statistical Validation](#5-statistical-validation)\n",
    "6. [PCA Shows Natural Clustering](#6-pca-shows-natural-clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/results')\n",
    "FIGURES_DIR = Path('../figures/paper')\n",
    "\n",
    "# Style\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    'cola': '#2ecc71',\n",
    "    'winogrande': '#3498db',\n",
    "    'snli': '#9b59b6',\n",
    "    'hellaswag': '#e74c3c',\n",
    "    'paws': '#e67e22',\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    'cola': 'CoLA (Grammar)',\n",
    "    'winogrande': 'WinoGrande (Commonsense)',\n",
    "    'snli': 'SNLI (Inference)',\n",
    "    'hellaswag': 'HellaSwag (Completion)',\n",
    "    'paws': 'PAWS (Paraphrase)',\n",
    "}\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load domain attribution metrics\n",
    "with open(DATA_DIR / 'domain_attribution_metrics.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data['samples'])\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"\\nSamples per task type:\")\n",
    "print(df['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def residualize(metric, length):\n",
    "    \"\"\"Remove length effect from metric.\"\"\"\n",
    "    slope, intercept, _, _, _ = linregress(length, metric)\n",
    "    return metric - (slope * length + intercept)\n",
    "\n",
    "def cohens_d(a, b):\n",
    "    \"\"\"Cohen's d effect size.\"\"\"\n",
    "    na, nb = len(a), len(b)\n",
    "    pooled_std = np.sqrt(((na-1)*np.std(a, ddof=1)**2 + (nb-1)*np.std(b, ddof=1)**2) / (na+nb-2))\n",
    "    if pooled_std == 0:\n",
    "        return 0\n",
    "    return (np.mean(a) - np.mean(b)) / pooled_std\n",
    "\n",
    "# Add residualized metrics\n",
    "metrics = ['n_active', 'mean_influence', 'top_100_concentration', 'mean_activation']\n",
    "for m in metrics:\n",
    "    df[f'{m}_resid'] = residualize(df[m].values, df['text_length'].values)\n",
    "\n",
    "print(\"Added length-controlled metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Five Task Types\n",
    "\n",
    "We analyze samples from five NLP benchmark tasks, each requiring different cognitive operations:\n",
    "\n",
    "| Dataset | Task Type | What It Tests |\n",
    "|---------|-----------|---------------|\n",
    "| **CoLA** | Grammar | Linguistic acceptability (\"Is this sentence grammatical?\") |\n",
    "| **WinoGrande** | Commonsense | Pronoun resolution requiring world knowledge |\n",
    "| **SNLI** | Inference | Natural language inference (entailment, contradiction, neutral) |\n",
    "| **HellaSwag** | Completion | Sentence completion requiring situational understanding |\n",
    "| **PAWS** | Paraphrase | Semantic similarity detection |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from each task type\n",
    "print(\"Example samples:\")\n",
    "print(\"=\" * 70)\n",
    "for src in ['cola', 'winogrande', 'snli', 'hellaswag', 'paws']:\n",
    "    example = df[df['source'] == src].iloc[0]['text']\n",
    "    print(f\"\\n{LABELS[src]}:\")\n",
    "    print(f\"  \\\"{example[:80]}...\\\"\" if len(example) > 80 else f\"  \\\"{example}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by task type\n",
    "summary = df.groupby('source').agg({\n",
    "    'text_length': 'mean',\n",
    "    'n_active': 'mean',\n",
    "    'mean_influence': 'mean',\n",
    "    'top_100_concentration': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['Avg Length', 'Avg N Active', 'Avg Influence', 'Avg Concentration']\n",
    "print(\"Raw metrics by task type:\")\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Raw vs Length-Controlled Comparison\n",
    "\n",
    "This is the critical comparison. Which signals are genuine, and which are artifacts of text length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the length control comparison figure\n",
    "fig_path = FIGURES_DIR / 'length_control_comparison.png'\n",
    "if fig_path.exists():\n",
    "    display(Image(filename=str(fig_path), width=800))\n",
    "else:\n",
    "    print(f\"Figure not found: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute effect sizes: Grammar vs Reasoning\n",
    "cola = df[df['source'] == 'cola']\n",
    "others = df[df['source'] != 'cola']\n",
    "\n",
    "print(\"Effect Sizes: Grammar (CoLA) vs Reasoning Tasks\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<25} {'Raw d':>12} {'Controlled d':>15} {'Change':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for m in metrics:\n",
    "    d_raw = cohens_d(cola[m].values, others[m].values)\n",
    "    d_resid = cohens_d(cola[f'{m}_resid'].values, others[f'{m}_resid'].values)\n",
    "    \n",
    "    if abs(d_resid) < 0.2 and abs(d_raw) > 0.8:\n",
    "        change = \"COLLAPSED\"\n",
    "    elif abs(d_resid) > 0.5:\n",
    "        change = \"PERSISTS\"\n",
    "    else:\n",
    "        change = \"\"\n",
    "    \n",
    "    print(f\"{m:<25} {d_raw:>+12.2f} {d_resid:>+15.2f} {change:>15}\")\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Key Finding\n",
    "\n",
    "**Before length control:**\n",
    "- n_active: d = 2.17 (HUGE difference)\n",
    "- mean_influence: d = 3.22 (HUGE difference)\n",
    "\n",
    "**After length control:**\n",
    "- n_active: d = 0.07 (COLLAPSED - was entirely length-driven)\n",
    "- mean_influence: d = 1.08 (PERSISTS - genuine signal)\n",
    "\n",
    "**Conclusion**: The real signal is in influence and concentration, not feature counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The Gradient from Focused to Diffuse\n",
    "\n",
    "Tasks don't just cluster into \"grammar\" vs \"everything else.\" There's a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display domain distributions figure\n",
    "fig_path = FIGURES_DIR / 'domain_distributions.png'\n",
    "if fig_path.exists():\n",
    "    display(Image(filename=str(fig_path), width=800))\n",
    "else:\n",
    "    print(f\"Figure not found: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank tasks by mean influence (length-controlled)\n",
    "task_profiles = df.groupby('source').agg({\n",
    "    'mean_influence_resid': 'mean',\n",
    "    'top_100_concentration_resid': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "task_profiles = task_profiles.sort_values('mean_influence_resid', ascending=False)\n",
    "task_profiles.columns = ['Influence (resid)', 'Concentration (resid)']\n",
    "\n",
    "print(\"Task types ranked by influence (length-controlled):\")\n",
    "print(\"=\" * 50)\n",
    "for src in task_profiles.index:\n",
    "    inf = task_profiles.loc[src, 'Influence (resid)']\n",
    "    conc = task_profiles.loc[src, 'Concentration (resid)']\n",
    "    print(f\"  {LABELS[src]:<30} Inf: {inf:+.4f}  Conc: {conc:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "**High influence + High concentration = Focused computation**\n",
    "- CoLA (grammar): Checking rules, few key features, strong connections\n",
    "\n",
    "**Low influence + Low concentration = Diffuse computation**\n",
    "- PAWS (paraphrase): Comparing meanings, many features, weak connections\n",
    "- HellaSwag (completion): Integrating context, distributed processing\n",
    "\n",
    "This matches intuition: grammar checking is \"focused\" (apply specific rules), while reasoning is \"diffuse\" (integrate many pieces of information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Statistical Validation\n",
    "\n",
    "Are these differences real, or could they be due to chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display significance boxplots\n",
    "fig_path = FIGURES_DIR / 'boxplots_significance.png'\n",
    "if fig_path.exists():\n",
    "    display(Image(filename=str(fig_path), width=800))\n",
    "else:\n",
    "    print(f\"Figure not found: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals\n",
    "def bootstrap_ci(group1, group2, n_boot=5000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    boot_ds = []\n",
    "    for _ in range(n_boot):\n",
    "        s1 = np.random.choice(group1, size=len(group1), replace=True)\n",
    "        s2 = np.random.choice(group2, size=len(group2), replace=True)\n",
    "        boot_ds.append(cohens_d(s1, s2))\n",
    "    return np.percentile(boot_ds, 2.5), np.percentile(boot_ds, 97.5)\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Intervals (5000 resamples)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'d':>10} {'95% CI':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for m in ['mean_influence_resid', 'top_100_concentration_resid', 'mean_activation_resid']:\n",
    "    d = cohens_d(cola[m].values, others[m].values)\n",
    "    ci_low, ci_high = bootstrap_ci(cola[m].values, others[m].values)\n",
    "    ci_str = f\"[{ci_low:.2f}, {ci_high:.2f}]\"\n",
    "    excludes_zero = \"*\" if ci_low > 0 or ci_high < 0 else \"\"\n",
    "    print(f\"{m.replace('_resid', ''):<25} {d:>+10.2f} {ci_str:>20} {excludes_zero}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"* = 95% CI excludes zero (robust effect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test\n",
    "def permutation_test(values, labels, n_perm=1000, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    grammar_mask = labels == 'cola'\n",
    "    observed_d = abs(cohens_d(values[grammar_mask], values[~grammar_mask]))\n",
    "    \n",
    "    null_ds = []\n",
    "    for _ in range(n_perm):\n",
    "        shuffled = rng.permutation(labels)\n",
    "        shuffled_mask = shuffled == 'cola'\n",
    "        null_ds.append(abs(cohens_d(values[shuffled_mask], values[~shuffled_mask])))\n",
    "    \n",
    "    return observed_d, np.mean(np.array(null_ds) >= observed_d)\n",
    "\n",
    "print(\"Permutation Test Results (1000 shuffles)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<30} {'Observed d':>12} {'p-value':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for m in ['mean_influence_resid', 'top_100_concentration_resid', 'n_active_resid']:\n",
    "    obs_d, p_val = permutation_test(df[m].values, df['source'].values)\n",
    "    sig = \"***\" if p_val < 0.001 else (\"**\" if p_val < 0.01 else (\"*\" if p_val < 0.05 else \"\"))\n",
    "    print(f\"{m.replace('_resid', ''):<30} {obs_d:>12.3f} {p_val:>12.4f} {sig}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"*** p<0.001, ** p<0.01, * p<0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. PCA Shows Natural Clustering\n",
    "\n",
    "Do task types naturally cluster in metric space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PCA clustering figure\n",
    "fig_path = FIGURES_DIR / 'pca_clustering.png'\n",
    "if fig_path.exists():\n",
    "    display(Image(filename=str(fig_path), width=700))\n",
    "else:\n",
    "    print(f\"Figure not found: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA on all metrics\n",
    "all_metrics = ['n_active', 'n_edges', 'mean_influence', 'max_influence', \n",
    "               'top_100_concentration', 'mean_activation', 'max_activation',\n",
    "               'logit_entropy', 'max_logit_prob']\n",
    "\n",
    "available = [m for m in all_metrics if m in df.columns]\n",
    "X = df[available].values\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "print(\"PCA Variance Explained:\")\n",
    "print(\"=\" * 40)\n",
    "for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
    "    print(f\"  PC{i}: {var:.1%}\")\n",
    "print(f\"  Total (3 PCs): {pca.explained_variance_ratio_.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "for src in ['cola', 'winogrande', 'snli', 'hellaswag', 'paws']:\n",
    "    mask = df['source'] == src\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               alpha=0.6, label=LABELS[src], \n",
    "               color=COLORS[src], s=60,\n",
    "               edgecolors='white', linewidth=0.5)\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax.set_title('Task Types Cluster in Attribution Metric Space')\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Found\n",
    "\n",
    "1. **Different tasks produce different activation patterns** - This is real, not an artifact.\n",
    "\n",
    "2. **The signal is in influence and concentration**, not feature counts:\n",
    "   - `mean_influence`: d = 1.08 (large effect, persists after length control)\n",
    "   - `concentration`: d = 0.87 (large effect, persists after length control)\n",
    "   - `n_active`: d = 0.07 (collapses - was just measuring text length)\n",
    "\n",
    "3. **Tasks form a gradient from focused to diffuse**:\n",
    "   - Grammar (CoLA): High influence, high concentration = focused computation\n",
    "   - Paraphrase (PAWS): Low influence, low concentration = diffuse computation\n",
    "\n",
    "4. **Statistical validation is strong**:\n",
    "   - Bootstrap 95% CIs exclude zero\n",
    "   - Permutation test p < 0.001\n",
    "\n",
    "### What This Means\n",
    "\n",
    "Activation topology measures **how** a model computes, not **what** it computes. Different cognitive operations (grammar checking vs. reasoning) require different internal structures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
