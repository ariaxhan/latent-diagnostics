{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: How We Measure Activation Topology\n",
    "\n",
    "**Series**: Latent Diagnostics Analysis (3 of 5)\n",
    "\n",
    "This notebook explains the methodology behind our measurements. What tools do we use? What do the numbers mean? How do we avoid confounding variables?\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Attribution Graphs](#1-attribution-graphs)\n",
    "2. [The Metrics We Extract](#2-the-metrics-we-extract)\n",
    "3. [Length Control via Residualization](#3-length-control-via-residualization)\n",
    "4. [Statistical Tests](#4-statistical-tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/results')\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Attribution Graphs\n",
    "\n",
    "### What Are They?\n",
    "\n",
    "Attribution graphs are **causal graphs** showing feature-to-feature influence inside a language model. Each node represents an interpretable feature (from transcoders), and each edge represents causal influence between features.\n",
    "\n",
    "### The Tool: circuit-tracer\n",
    "\n",
    "We use [circuit-tracer](https://github.com/anthropics/circuit-tracer) from Anthropic, combined with [Goodfire's transcoders](https://github.com/goodfire-ai/goodfire-transcoders) for Gemma 2.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Load a \"replacement model\" where MLPs are swapped for transcoder features\n",
    "2. Run text through the model\n",
    "3. Track which features activate and how they influence each other\n",
    "4. Output: An **attribution graph** with:\n",
    "   - Nodes = transcoder features (interpretable units)\n",
    "   - Edges = causal influence between features\n",
    "\n",
    "```python\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "\n",
    "model = ReplacementModel.from_pretrained(\n",
    "    \"google/gemma-2-2b\",      # Base model\n",
    "    \"gemma\",                   # Transcoder set\n",
    "    dtype=torch.bfloat16,\n",
    "    device=torch.device(\"cuda\")\n",
    ")\n",
    "\n",
    "text = \"The cat sat on the mat.\"\n",
    "graph = attribute(text, model)\n",
    "```\n",
    "\n",
    "### What We Get Back\n",
    "\n",
    "| Attribute | Type | What It Contains |\n",
    "|-----------|------|------------------|\n",
    "| `graph.active_features` | list | IDs of features that fired |\n",
    "| `graph.activation_values` | tensor | Activation strength of each feature |\n",
    "| `graph.adjacency_matrix` | tensor | Causal influence: `adj[i,j]` = how much feature i contributes to feature j |\n",
    "| `graph.logit_probabilities` | tensor | Output probability distribution |\n",
    "\n",
    "### Our Contribution\n",
    "\n",
    "Anthropic built the tools. We built:\n",
    "- **Summary metrics** extracted from these graphs\n",
    "- **The research question**: Do metrics differ by task type?\n",
    "- **Statistical analysis**: Length control, bootstrap CIs, permutation tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Metrics We Extract\n",
    "\n",
    "From each attribution graph, we extract summary statistics. These reduce the complex graph to a few interpretable numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data to see the actual schema\n",
    "with open(DATA_DIR / 'domain_attribution_metrics.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Dataset: {data['metadata']['n_samples']} samples\")\n",
    "print(\"\\nSample structure:\")\n",
    "sample = data['samples'][0]\n",
    "for key, value in sample.items():\n",
    "    if key != 'text':\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Definitions\n",
    "\n",
    "| Metric | What It Measures | How It's Computed |\n",
    "|--------|------------------|-------------------|\n",
    "| `n_active` | Feature count | Number of features with non-zero activation |\n",
    "| `mean_activation` | Avg feature strength | Mean of \\|activation\\| across active features |\n",
    "| `max_activation` | Peak feature strength | Maximum single feature activation |\n",
    "| `n_edges` | Connection count | Edges with influence > 0.01 threshold |\n",
    "| `mean_influence` | Avg edge weight | Mean of \\|adj[i,j]\\| across all pairs |\n",
    "| `max_influence` | Peak causal connection | Maximum single edge weight |\n",
    "| `top_100_concentration` | Influence Gini | Fraction of total influence in top 100 edges |\n",
    "| `logit_entropy` | Output uncertainty | Entropy of output probability distribution |\n",
    "| `max_logit_prob` | Output confidence | Probability of most likely next token |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(data['samples'])\n",
    "df['text_length'] = df['text'].str.len()\n",
    "\n",
    "# Check correlations with text length\n",
    "metrics = ['n_active', 'mean_influence', 'top_100_concentration', 'mean_activation']\n",
    "\n",
    "print(\"Correlation with text length:\")\n",
    "print(\"=\" * 50)\n",
    "for m in metrics:\n",
    "    r = np.corrcoef(df[m], df['text_length'])[0, 1]\n",
    "    robust = \"YES\" if abs(r) < 0.6 else \"NO\"\n",
    "    print(f\"  {m:<25} r = {r:+.3f}  Robust: {robust}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Robustness Table\n",
    "\n",
    "| Metric | What It Measures | Robust to Length? |\n",
    "|--------|------------------|-------------------|\n",
    "| n_active | Feature count | **NO** (r=0.96) |\n",
    "| mean_influence | Avg edge weight | YES |\n",
    "| concentration | Influence Gini | YES |\n",
    "| mean_activation | Avg feature strength | YES |\n",
    "\n",
    "**Critical insight**: `n_active` is almost perfectly correlated with text length. Longer text = more tokens = more features fire. This metric cannot be used directly for task comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Length Control via Residualization\n",
    "\n",
    "### The Problem\n",
    "\n",
    "`n_active` scales with text length (r = 0.96). Different tasks have different average text lengths:\n",
    "- Grammar examples (CoLA): ~40 characters\n",
    "- Reasoning examples (HellaSwag): ~175 characters\n",
    "\n",
    "If we compare raw `n_active` between tasks, we're just measuring **length**, not **task differences**.\n",
    "\n",
    "### The Solution: Residualization\n",
    "\n",
    "Regress each metric on text_length, use residuals.\n",
    "\n",
    "**How it works:**\n",
    "1. Fit a line: `metric = slope * length + intercept`\n",
    "2. Compute residuals: `residual = actual - predicted`\n",
    "3. Use residuals for analysis\n",
    "\n",
    "The residual represents \"the part of the metric that isn't explained by length.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residualize(metric, length):\n",
    "    \"\"\"\n",
    "    Remove the effect of text length from a metric.\n",
    "    \n",
    "    Parameters:\n",
    "        metric: array of metric values\n",
    "        length: array of text lengths\n",
    "    \n",
    "    Returns:\n",
    "        residuals: metric values with length effect removed\n",
    "    \"\"\"\n",
    "    slope, intercept, _, _, _ = linregress(length, metric)\n",
    "    predicted = slope * length + intercept\n",
    "    return metric - predicted\n",
    "\n",
    "# Example: residualize n_active\n",
    "n_active = df['n_active'].values\n",
    "lengths = df['text_length'].values\n",
    "\n",
    "n_active_resid = residualize(n_active, lengths)\n",
    "\n",
    "# Verify: residuals should have zero correlation with length\n",
    "r_before = np.corrcoef(n_active, lengths)[0, 1]\n",
    "r_after = np.corrcoef(n_active_resid, lengths)[0, 1]\n",
    "\n",
    "print(\"Residualization example (n_active):\")\n",
    "print(f\"  Correlation with length BEFORE: r = {r_before:.3f}\")\n",
    "print(f\"  Correlation with length AFTER:  r = {r_after:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to all metrics\n",
    "for m in metrics:\n",
    "    df[f'{m}_resid'] = residualize(df[m].values, df['text_length'].values)\n",
    "\n",
    "# Verify all residuals have zero correlation with length\n",
    "print(\"Verification: All residuals have zero correlation with length\")\n",
    "print(\"=\" * 60)\n",
    "for m in metrics:\n",
    "    r = np.corrcoef(df[f'{m}_resid'], df['text_length'])[0, 1]\n",
    "    print(f\"  {m}_resid: r = {r:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Statistical Tests\n",
    "\n",
    "We use three complementary methods to validate our findings.\n",
    "\n",
    "### Cohen's d (Effect Size)\n",
    "\n",
    "**What it measures**: How different two groups are, in standard deviation units.\n",
    "\n",
    "```\n",
    "d = (mean1 - mean2) / pooled_standard_deviation\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "| d | Interpretation |\n",
    "|---|----------------|\n",
    "| 0.2 | Small effect |\n",
    "| 0.5 | Medium effect |\n",
    "| 0.8 | Large effect |\n",
    "| > 1.0 | Very large effect |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(a, b):\n",
    "    \"\"\"\n",
    "    Compute Cohen's d effect size between two groups.\n",
    "    \n",
    "    Parameters:\n",
    "        a, b: arrays of values for each group\n",
    "    \n",
    "    Returns:\n",
    "        d: standardized effect size\n",
    "    \"\"\"\n",
    "    na, nb = len(a), len(b)\n",
    "    pooled_std = np.sqrt(\n",
    "        ((na-1)*np.std(a, ddof=1)**2 + (nb-1)*np.std(b, ddof=1)**2) / (na+nb-2)\n",
    "    )\n",
    "    if pooled_std == 0:\n",
    "        return 0\n",
    "    return (np.mean(a) - np.mean(b)) / pooled_std\n",
    "\n",
    "# Example: Grammar vs Reasoning\n",
    "cola = df[df['source'] == 'cola']\n",
    "others = df[df['source'] != 'cola']\n",
    "\n",
    "print(\"Effect sizes: Grammar (CoLA) vs Reasoning\")\n",
    "print(\"=\" * 50)\n",
    "for m in metrics:\n",
    "    d_raw = cohens_d(cola[m].values, others[m].values)\n",
    "    d_resid = cohens_d(cola[f'{m}_resid'].values, others[f'{m}_resid'].values)\n",
    "    print(f\"  {m:<25} Raw: d={d_raw:+.2f}  Controlled: d={d_resid:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals\n",
    "\n",
    "**What it measures**: How precise is our effect size estimate?\n",
    "\n",
    "**Method**:\n",
    "1. Resample data with replacement (5000 times)\n",
    "2. Compute Cohen's d for each resample\n",
    "3. Take 2.5th and 97.5th percentiles as 95% CI\n",
    "\n",
    "**Interpretation**: If 95% CI excludes zero, the effect is robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(group1, group2, n_boot=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Bootstrap 95% confidence interval for Cohen's d.\n",
    "    \n",
    "    Parameters:\n",
    "        group1, group2: arrays of values\n",
    "        n_boot: number of bootstrap resamples\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        (ci_low, ci_high): 95% confidence interval\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    boot_ds = []\n",
    "    \n",
    "    for _ in range(n_boot):\n",
    "        sample1 = np.random.choice(group1, size=len(group1), replace=True)\n",
    "        sample2 = np.random.choice(group2, size=len(group2), replace=True)\n",
    "        boot_ds.append(cohens_d(sample1, sample2))\n",
    "    \n",
    "    return np.percentile(boot_ds, 2.5), np.percentile(boot_ds, 97.5)\n",
    "\n",
    "# Example: Bootstrap CI for influence\n",
    "ci_low, ci_high = bootstrap_ci(\n",
    "    cola['mean_influence_resid'].values,\n",
    "    others['mean_influence_resid'].values\n",
    ")\n",
    "print(f\"Influence (length-controlled): 95% CI = [{ci_low:.2f}, {ci_high:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test (Shuffle Test)\n",
    "\n",
    "**What it measures**: Could the observed effect be due to random chance?\n",
    "\n",
    "**Method**:\n",
    "1. Shuffle task labels randomly (1000 times)\n",
    "2. Compute Cohen's d for each shuffle (null distribution)\n",
    "3. p-value = proportion of shuffles with d >= observed\n",
    "\n",
    "**Interpretation**: If p < 0.05, the effect is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(values, labels, n_permutations=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Permutation test for difference between groups.\n",
    "    \n",
    "    Parameters:\n",
    "        values: array of metric values\n",
    "        labels: array of group labels\n",
    "        n_permutations: number of shuffles\n",
    "    \n",
    "    Returns:\n",
    "        observed_d: actual effect size\n",
    "        p_value: proportion of shuffles >= observed\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    # Observed effect\n",
    "    grammar_mask = labels == 'cola'\n",
    "    observed_d = abs(cohens_d(values[grammar_mask], values[~grammar_mask]))\n",
    "    \n",
    "    # Null distribution\n",
    "    null_ds = []\n",
    "    for _ in range(n_permutations):\n",
    "        shuffled = rng.permutation(labels)\n",
    "        shuffled_mask = shuffled == 'cola'\n",
    "        null_ds.append(abs(cohens_d(values[shuffled_mask], values[~shuffled_mask])))\n",
    "    \n",
    "    p_value = np.mean(np.array(null_ds) >= observed_d)\n",
    "    return observed_d, p_value\n",
    "\n",
    "# Example: Permutation test for influence\n",
    "obs_d, p_val = permutation_test(\n",
    "    df['mean_influence_resid'].values,\n",
    "    df['source'].values\n",
    ")\n",
    "print(f\"Influence: observed d = {obs_d:.3f}, p = {p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### The Pipeline\n",
    "\n",
    "1. **Input**: Text samples from different task types\n",
    "2. **Tool**: circuit-tracer + Goodfire transcoders\n",
    "3. **Output**: Attribution graphs (nodes = features, edges = influence)\n",
    "4. **Extraction**: Summary metrics (n_active, mean_influence, concentration, etc.)\n",
    "5. **Length Control**: Residualization removes length confound\n",
    "6. **Validation**: Cohen's d, bootstrap CIs, permutation tests\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "`n_active` is confounded by length (r=0.96). After length control:\n",
    "- `n_active` collapses (d: 2.17 -> 0.07)\n",
    "- `mean_influence` persists (d: 3.22 -> 1.08)\n",
    "- `concentration` persists (d: 2.36 -> 0.87)\n",
    "\n",
    "The real signal is in **influence** and **concentration**, not feature counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
